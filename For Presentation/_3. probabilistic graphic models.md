## Thomas L3 - Probabilistic Graphical Models

-  Probabilistic models (syntax and semantics)
-  Inference in probabilistic graphical models
-  Maximum likelihood learning
-  The EM algorithm

A **probabilistic model** is typically learned from data using **statistical learning techniques**. They typically apply **Probabilistic inference** algorithms to use models for prediction or structure analysis. **Probabilitis inference** is the task of deriving the probability of one or more random variables taking a specific cvalue or set of values.

 Now a **Probabilistic Graphical Model** are a type of graphical model that is used to model the dynamics of a system:
- supports a structured specification of high-dimensional distributions in terms of low-dimensional factors
- its structured representation can be exploited for efficient learning and inference algorithms (sometimes ...)
- its graphical representation gives a human-friendly design and description possibilities

A probabilistic graphical model two different types of variables these are: 
- **State variables:** variables that are used to describe the state of the system at each point in time.
- **Observation variables:** describe what can be observed from the system at each point in time.

Some Advantages of probabilistic methods:
- Principled quantification of prediction uncertainties
- Robust and principled techniques to deal with incomplete information, missing data. 
- Provides a general model of the domain that can in principle be used to answer any inference query.
- Enables integration of domain knowledge. 
- Transparent and explainable.
### Bayesian Networks

**Syntax:** Bayesian network is a collection of variables, say $X1...Xn$, that has a graphical component, **a directed acyclic graph** $G = {V, E}$ where the nodes correspond to the variables $X1,..., Xn$. We have a set of local conditional distributions, one for each variable in the network, where we have the probability of that variable conditioned in it's parents: $P = {p(Xi | pa(Xi)), Xi \epsilon V}$ where $pa(Xi)$ are the parents of $Xi$ in $G$ as defined by the edges $E$. 

**Semantics:** We have bayesian network over a collection of nodes $X1,..., Xn$ specify a join probability distribution given by the multiplication of conditional probabibility distributions that goes into the specification of the model. 
$pN(X1,...Xn) = \Pi_{i=1}^n.p(Xi | pa(Xi))$ 

**Inference:** Given a bayesian network model, and some evidence, we can do inference on it. 
$p(X = x, G = g) = \frac{p(X = x, G = g)}{p(g)}$ Probability of X, given G = g. It's done by finding the probability of two variables of interest $X$ and $G$, and dividing with probability of component that we are conditioning on, $G$ in this case. 

The maximum likelihood learning is probabilistic approach to determine values fro parameters of a model. The goal is to find the parameters that maximize the likelihood of the data. To sum up maximum likelihood learning is A technic to find the parameters that maximize the likelihood of the data. 

### **Parameter estimation**
First of all **Maximum Likelihood Estimation** is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.

If we for example use Gaussian distribution which has 2 parameters, mean and the standard deviation. Different values of these parameters result in diffrent models. Then **Maximum Likelihood Estimation** can be used to find the values that best fit the data. 
**To find the maximum likelihood estimate for parameters in a Bayesian network model. We find the maximum likelihood estimates for each conditional probability distribution in the model. In general with complete data you get a maximum likelihood estimate as the fraction of counts over the total number of counts.**

We are looking for the $\theta$ that maximizes the probabibility in the thumbtack example. It is expressed as the $arg.max(log p(data|\theta))$. 

The probabibility could be very small that leads to numerical instability. So we use the log, which makes it a bit more stable, and sometimes make derivation easier. On the graph with theta in x axis and log in y axis, we can find the unique global maximum by finding the derivative and equating it to 0, because slope at maximum value would be 0. 

<img src="..\..\attachments\maxlike.png" width="400px">
<img src="..\..\attachments\maxlike1.png" width="400px">

In general, with complete data, you get a max likelihood estimate as the fraction of counts over the total no of counts. 

If some value has probability 0, for future data it has worst possible fit. If we for example have some data where some element only does not occour, but the data is limimted. To avoid overfitting, initialize all tables with pseudo-counts (corresponding to virtual data). E.g. use a pseudo count of 1, then add the actual counts from data. Now normalize. This eliminates the drawback of having a count being 0 and then giving 0 probability.

Now the **Expectation-Maximization (EM) algorithm:** is a general algorithm for finding maximum likelihood estimates for a set of parameters $\theta$ when the data are incomplete. Imagine if we want to find cluster in a dataset, but we do not know where each datapoint came from. 
We start with a random guess for what the distribution or clusters are and the proceed to improve iteratively by alternating two steps:
1. **Expectation:** Assign each data point to a cluster probabilistically. This is a hypothetical completionm of the data set obtained from the current estimate of the paramters. (The model is estimated based on the data)
2. **Maximazation:** Update the parameters for each cluster based on the points in the cluster. i.e. apply maximum lielihood inference to completed data to obtain new estimate for parameters. (the model is updated based on the estimated parameters)
