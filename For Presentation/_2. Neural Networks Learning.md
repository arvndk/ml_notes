## Thomas L2 - Neural Networks - Learning

- The basis for learning neural networks (loss functions and gradient decent)
- Learning and back propagation (computational graphs, the chain rule)
- Learning in practice (stochastic gradient descent, momentum, regularisation)

Neural networks have neurons as basic building blocks. 
Neural networks are often said to be analogous to human brain, but instead think of it more as a function approximator.  
Generally, they have a two-step computation:
1. Combine inputs as weighted sum. 
2. Computes output by activation function of combined inputs. 

**Activation functions** define how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.
 - **Sigmoid**: Output is between 0 and 1. 
 - **Hyberbolic tangent**: Output between -1 and 1.
 - **ReLU(Rectified Linear Unit)**: 0, when less than 0. z when z > 1.

A **loss function** measures how well the neural network performs on training data. We may seek to maximize or minimize the objective function, meaning that we are searching for a candidate solution that has the highest or lowest score respectively. Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply ‚Äúloss.‚Äù

 - **Squared loss**: Calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The loss value is minimized, although it can be used in a maximization optimization process by making the score negative.
 - **Log loss**: Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0). For a binary or two class prediction problem is actually calculated as the average cross entropy across all examples.
 - **Cross-entropy**: Cross-entropy and log loss are essentially the same; usually, we use the term log loss for binary classification problems, ie; when there are just two possible outcomes that can be either 0 or 1. Cross entropy is usually used when there are three or more possible outcomes.

### Gradient Descent

The standard approach for optimizing neural networks is using gradient-based methods. Simple gradient descent is an example of one such approach. 
1. Pick a random starting point w in the graph. 
2. Calculate the gradient(loss). 
3. Move in the opposite direction of the gradient: w‚Äô = w ‚Äì Œ∑ ‚àáloss(w)
    1. Œ∑ is the learning rate. 
    2. ‚àáloss(w) is the gradient of loss function. 
4. Do this until convergence. 

Calculating the gradient is computationally costly. It grows linearly with the sample size. Hence it can be rather time consuming with large datasets. One way to overcome this is by using a small batch of random samples of size B instead of using all the N samples. If batch size is 1, then it is known as **stochastic gradient descent**. Otherwise, it is called a **mini-batch gradient descent**. 

Ultimately we usually end up in a local minimum which is a point where the value is lower than at all neigboring points, so it is no longer possible to decrease the value by making infinitesimal steps. A global minimum is the absolute lowest value. As it can be difficult to find this point, we usually settle with a point wihch is very low, but not neccesarily the lowest. The **learning rate** is a positive scalar determining the size of the step. Which usually decreases over time to ensure convergence. 

Standard error of approximation is calculated as $œÉ/‚àöN$. If we take two batches of 100 and 10,000 samples and find the error, we will see that the error according to the equation is only less by a factor of 10 although it is 100 times slower. To get unbiased estimate, i.e., estimate of gradient in expectation should be equal to true gradient, then mini batches should be picked at random and independent to each other. This means this is only possible at first pass of data. 

### Regularization
Employed during training to avoid overfitting. It introduces a penalty for the norm of the parameters. Regularization basically adds the penalty as model complexity increases. It penalizes solutions with extreme values and produces smoother functions. In neural networks, typically only the weights (not the biases) are penalized. 

**L2 regularization**: $w = (1 - Œ∑a)w - Œ∑ ‚àáwL$

 Before updating weights, we scale current weights by factor 1-Œ∑Œ± where Œ± is the penalty factor. L2 penalizes sum of square weights, and drives the weights towards the origin, evenly. Is useful when you have codependent features.


**L1 Regularization**: 
$w = w - ansign(w) - Œ∑ ‚àáwL$

 sign is positive if w is positive and negative if w is negative. L1 regularization has tendency to drive weight towards 0, hence it has tendency to produce sparsely connected network structures. **L1 penalizes sum of absolute value of weights, and drives the weights towards zero. Is useful for feature selection, as we can drop variables associated with coefficient that go to zero.**

### Dropout
**Dropout refers to how a neural network can choose to ignore neurons during training, chosen at random.** Ignoring refers to the network not considering these neurons during a particular forward and backward pass. At each training stage, individual neurons are either dropped out of the network with some probability, so a reduced network is left. **It is done to avoid overfitting.**


### Back Propagation
Assume a neural network with three layers: input layer, hidden layer, and output layer. 
Training examples only specify the relationship between input values going to input layers and output values coming from output layers. But there are **no such direct observations about the hidden layer** from training examples. **Back propogation is a technique which can be used to compute the loss of neural networks**. It is a simple implementation of chain rule of derivatives.

<img src="..\..\attachments\chainrule.png" width="300px">

$ùõøf$ is the loss function. Output provided by a neural network also depends on what goes inside the neural network, at the input layer for example. The $g(x)$ denotes that. So, if we want to see how gradient looks like relative to a weight in the network, $g(x)$ in this case, we use the above chain rule. The RHS of the above equation basically means: Partial derivative of f with respect to $g(x)$ multiplied with partial derivative of $g(x)$ with respect to $x$.  **Our goal is to find gradients at each node with respect to the error at the final node. We start from the final node.**

<img src="..\..\attachments\backp1.png" width="500px">

Consider the computational graph above. Sample values are given in blue. 
 - $x1$ and $x2$ are the input layers.
 - $w1$ and $w2$ are weights. 
 - Now we will perform back propagation on the above graph. 
 - We want to be able to compute the partial derivative of the error with respect to any node in our graph.  


<img src="..\..\attachments\backp2.png" width="500px">

Node err:
Here the gradient with respect to the error is 1 since it gets differentiated with respect to itself. (Notice the green stuff at the very right which gets equated to 1). 
Node y6:
Gradient with respect to error is ùõøerr/ùõøy6. Results in 4.7949.

### Momentum
Sometimes it can take very long for a neural network to converge. Hence it is desirable to accelerate the learning process. Side effect is that it can help remove local minima. **Momentum** introduces a variable which captures the velocity of the system or the direction you are moving in. Instead of just updating weights based on gradient like we did before, we also add a fraction of existing velocity to the gradient. 

<img src="..\..\attachments\momentum1.png" width="400px">

The $Œ±v$ in above equation denotes the fraction of existing velocity. At each timestep, we add a fraction of velocity calculated in the previous timestep to the gradient. v is exponentially decaying.

<img src="..\..\attachments\nesterov.png" width="400px">

Above equation is a variant momentum named ‚ÄòNesterov momentum‚Äô where gradient is evaluated after current velocity is applied. 

