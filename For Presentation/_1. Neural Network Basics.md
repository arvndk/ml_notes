## Thomas L1 - Neural Networks - Basics
-    Neural network basics
-    Loss functions
-    Learning neural networks (gradient decent and stochadtic gradient descent)
-    Neural network structures
-    
Neural networks have neurons as basic building blocks. 
Neural networks are often said to be analogous to human brain, but instead think of it more as a function approximator.  
Generally, they have a two-step computation:
1. Combine inputs as weighted sum. 
2. Computes output by activation function of combined inputs. 

**Activation functions** define how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.
 - **Sigmoid**: Output is between 0 and 1. 
 - **Hyberbolic tangent**: Output between -1 and 1.
 - **ReLU(Rectified Linear Unit)**: 0, when less than 0. z when z > 1.

A **loss function** measures how well the neural network performs on training data. We may seek to maximize or minimize the objective function, meaning that we are searching for a candidate solution that has the highest or lowest score respectively. Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply “loss.”

 - **Squared loss**: Calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The loss value is minimized, although it can be used in a maximization optimization process by making the score negative.
 - **Log loss**: Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0). For a binary or two class prediction problem is actually calculated as the average cross entropy across all examples.
 - **Cross-entropy**: Cross-entropy and log loss are essentially the same; usually, we use the term log loss for binary classification problems, ie; when there are just two possible outcomes that can be either 0 or 1. Cross entropy is usually used when there are three or more possible outcomes.

### Gradient Descent

The standard approach for optimizing neural networks is using gradient-based methods. Simple gradient descent is an example of one such approach. 
1. Pick a random starting point w in the graph. 
2. Calculate the gradient(loss). 
3. Move in the opposite direction of the gradient: w’ = w – η ∇loss(w)
    1. η is the learning rate. 
    2. ∇loss(w) is the gradient of loss function. 
4. Do this until convergence. 

Now we introduce mini batches. A mini batch means that we only take a subset of the data in one iteration. If the batch size is 1 so we only look at one traing example in each iteration it is called **Stochastic Gradient Descent**. If we instead have a match size of e.g. 32 the it is reffered to as **mini-batch gradient descent.**

Ultimately we usually end up in a local minimum which is a point where the value is lower than at all neigboring points, so it is no longer possible to decrease the value by making infinitesimal steps. A global minimum is the absolute lowest value. As it can be difficult to find this point, we usually settle with a point wihch is very low, but not neccesarily the lowest. The **learning rate** is a positive scalar determining the size of the step. Which usually decreases over time to ensure convergence. 

In the exercise we experimented with different learning rates and batch sizes. Here we saw that a batchsize of 32 and 64 made little difference, but larger seems slightly better. For the learning rate 0.02 seemed good, as a larger one had a negativ impact and a smaller one made the converges slower. We also tried adding **momentum**. Sometimes it can take very long for a neural network to converge. Hence it is desirable to accelerate the learning process. Side effect is that it can help remove local minima. **Momentum** introduces a variable which captures the velocity of the system or the direction you are moving in. Instead of just updating weights based on gradient like we did before, we also add a fraction of existing velocity to the gradient. 

<img src="..\..\attachments\momentum1.png" width="400px">

The αv in above equation denotes the fraction of existing velocity. At each timestep, we add a fraction of velocity calculated in the previous timestep to the gradient. v is exponentially decaying.

<img src="..\..\attachments\nesterov.png" width="400px">

Above equation is a variant momentum named ‘Nesterov momentum’ where gradient is evaluated after current velocity is applied. 

